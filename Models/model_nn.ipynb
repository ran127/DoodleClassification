{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note filepaths\n",
    "PATH_TRAIN_LABELS = '../datasets/train_labels.csv'\n",
    "PATH_TRAIN_RAW = '../datasets/train_images.npy'\n",
    "PATH_TEST_RAW = '../datasets/test_images.npy'\n",
    "PATH_TRAIN = '../datasets/train_images_preprocessed.npy'\n",
    "PATH_TEST = '../datasets/train_images_preprocessed.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note constants\n",
    "np.random.seed(19) \n",
    "IMG_SIZE = 35\n",
    "K = 4 # number of folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(pd.read_csv(PATH_TRAIN_LABELS, delimiter=\",\", header=0, index_col=0))\n",
    "train_images = np.load(PATH_TRAIN, encoding=\"latin1\")\n",
    "test_images = np.load(PATH_TEST, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS DATA TO DESIRED FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding/decoding\n",
    "cat0 = sorted(['sink','pear','moustache','nose','skateboard','penguin','peanut','skull','panda',\n",
    "        'paintbrush', 'nail','apple','rifle','mug','sailboat','pineapple','spoon','rabbit',\n",
    "        'shovel','rollerskates','screwdriver','scorpion','rhinoceros','pool','octagon',\n",
    "        'pillow','parrot','squiggle','mouth','empty','pencil'])\n",
    "cat1 = {i: cat0[i] for i in range(len(cat0))}\n",
    "cat2 = {cat0[i]: i for i in range(len(cat0))}\n",
    "print(cat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "input_y = np.array([cat2[train_labels[i][0]] for i in range(len(train_labels))])\n",
    "input_x = np.array([train_images[:,1][i] for i in range(len(train_images))], dtype=float)\n",
    "output_x = np.array([test_images[:,1][i] for i in range(len(test_images))], dtype=float)\n",
    "input_x /= 255.0\n",
    "output_x /= 255.0\n",
    "input_data = np.array([[input_x[i].tolist(), input_y[i].tolist()] for i in range(len(input_x))])\n",
    "output_data = np.array([[output_x[i].tolist(), int(0)] for i in range(len(output_x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 80/10/10 splits\n",
    "def create_splits(data, k):\n",
    "    data = data[np.random.permutation(len(data))]\n",
    "    # create test\n",
    "    n = data.shape[0]\n",
    "    test = data[int(np.ceil(k*n/(k+1))):int(np.ceil((k+1)*n/(k+1)))]\n",
    "    test_x = np.array([test[:,0][j] for j in range(len(test[:,0]))], dtype=float)\n",
    "    test_y = np.array([[test[:,1][j]] for j in range(len(test[:,1]))], dtype=float)\n",
    "    # create train and valid\n",
    "    n = data.shape[0]-test.shape[0]\n",
    "    train_x, train_y, valid_x, valid_y = [], [], [], []\n",
    "    for i in range(k):\n",
    "        t = data[int(np.ceil(i*n/k)):int(np.ceil((i+1)*n/k))]\n",
    "        v = data[np.delete(np.arange(0,n),np.arange(int(np.ceil(i*n/k)),int(np.ceil((i+1)*n/k))))]\n",
    "        v_x = np.array([v[:,0][j] for j in range(len(v[:,0]))], dtype=float)\n",
    "        v_y = np.array([v[:,1][j] for j in range(len(v[:,1]))], dtype=float)\n",
    "        t_x = np.array([t[:,0][j] for j in range(len(t[:,0]))], dtype=float)\n",
    "        t_y = np.array([t[:,1][j] for j in range(len(t[:,1]))], dtype=float)\n",
    "        valid_x.append(v_x)\n",
    "        valid_y.append(v_y)\n",
    "        train_x.append(t_x)\n",
    "        train_y.append(t_y)\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for cross validation\n",
    "train_x, train_y, valid_x, valid_y, test_x, test_y = create_splits(input_data, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x[0].shape)\n",
    "print(valid_x[0].shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "list_rates = [5e-1] # learning rate\n",
    "list_nodes = [250, 350, 450, 550, 650, 750, 850, 950] # number of nodes per layer\n",
    "list_layers = [1, 2, 3, 4] # number of hidden layers\n",
    "batch_size = [1/100, 1/10, 1]\n",
    "activation = ['sigmoid', 'tanh', 'relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "# make a matrix \n",
    "def matrix(m, n, fill=0.0):\n",
    "    return np.zeros(shape=(m,n)) + fill\n",
    "\n",
    "# make a random matrix\n",
    "def rand_matrix(m, n, a=0, b=1):\n",
    "    return np.random.rand(m, n) * (b - a) + a\n",
    "\n",
    "# use logistic regression loss function \n",
    "def loss_fn(predict, truth):\n",
    "    n = len(truth)\n",
    "    loss = (- 1 / n) * np.sum(truth * np.log(predict) + (1 - truth) * (np.log(1 - predict)))\n",
    "    loss = np.squeeze(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, ni, nh, no, lr):\n",
    "        self.lr = lr\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "        \n",
    "        # bias vectors \n",
    "        # self.bh = np.zeros((1, self.nh))\n",
    "        # self.bo = np.zeros((1, self.no))\n",
    "        self.bh = np.ones(self.nh)\n",
    "        self.bo = np.ones(self.no)\n",
    "\n",
    "        # create weights\n",
    "        # default to range (-0.5, 0.5)\n",
    "        self.wh = rand_matrix(self.ni, self.nh, -0.5, 0.5)\n",
    "        self.wo = rand_matrix(self.nh, self.no, -0.5, 0.5)\n",
    "    \n",
    "    # training feed forward, obtain output from weight matrices and bias vectors\n",
    "    def propagate(self, inputs):\n",
    "        self.ai = inputs\n",
    "        # ---- can add loop here for more hidden layers ----\n",
    "        # hidden layers activations\n",
    "        self.ah = np.dot(self.ai, self.wh) + self.bh\n",
    "        # hidden output \n",
    "        self.oh = np.tanh(self.ah)\n",
    "        # output layers activations\n",
    "        self.ao = np.dot(self.ah, self.wo) + self.bo  \n",
    "        #h output layers output \n",
    "        self.oo = sigmoid(self.ao)\n",
    "\n",
    "    # training back propagation, updates neural network's weight matrices and bias vectors\n",
    "    def backPropagate(self, x, y, eta):\n",
    "        n = x.shape[0]\n",
    "        self.dao = self.oo - y\n",
    "        self.dwo = np.dot(self.oh.T, self.dao) / n\n",
    "        self.dbo = np.sum(self.dao) / n\n",
    "        self.dah = np.dot(self.dao, self.wo.T)*(1-np.tanh(self.ah))\n",
    "        self.dwh = np.dot(x.T, self.dah) / n\n",
    "        self.dbh = np.sum(self.dah) / n\n",
    "        # update weights using gradient descent method. learning rate = eta\n",
    "        self.wo = self.wo - eta * self.dwo\n",
    "        self.wh = self.wh - eta * self.dwh\n",
    "        self.bo = self.bo - eta * self.dbo\n",
    "        self.bh = self.bh - eta * self.dbh \n",
    "        \n",
    "    def predict(self, x):\n",
    "        ah = np.dot(x, self.wh) + self.bh\n",
    "        # hidden layers output \n",
    "        oh = np.tanh(ah)\n",
    "        # output layers activations\n",
    "        ao = np.dot(ah, self.wo) + self.bo  \n",
    "        # output layers output \n",
    "        oo = sigmoid(ao)\n",
    "        return oo\n",
    "      \n",
    "    # takes in Y     \n",
    "    def train(self, X, Y, iterations = 1000):\n",
    "        trend = []\n",
    "        eta = self.lr\n",
    "        # create output matrix\n",
    "        Y_m = np.zeros((X.shape[0], 31))\n",
    "        for i in range(len(Y)):\n",
    "            Y_m[i][int(Y[i])] = 1\n",
    "        for i in range(iterations):\n",
    "            output = self.propagate(X)\n",
    "            self.backPropagate(X, Y_m, eta)\n",
    "            pred = np.argmax(self.oo, axis=1)\n",
    "            loss = loss_fn(self.oo, Y_m)\n",
    "            diff = Y - pred\n",
    "            acc = (diff == 0).sum() / len(Y)\n",
    "            if( i % (iterations / 100) == 0): \n",
    "                trend.append([acc, loss])\n",
    "                print(\"iteration\", i, \":\\t\", \"train acc:\", acc)\n",
    "        return trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models = []\n",
    "for r in list_rates:\n",
    "    for n in list_nodes:\n",
    "        list_models.append(NN(ni=1225, nh=n, no=31, lr=r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pred = (0, 0, []) # [accuracy, nn, training trend]\n",
    "trends = []\n",
    "\n",
    "for nn in list_models:\n",
    "    for i in range(K):\n",
    "        t_x, t_y, v_x, v_y = train_x[i], train_y[i], valid_x[i], valid_y[i]\n",
    "        res = nn.train(t_x, t_y, 2000)\n",
    "        # validate with validation set after the training\n",
    "        v_o = nn.predict(v_x)\n",
    "        pred = np.argmax(v_o, axis=1)\n",
    "        diff = v_y - pred\n",
    "        acc = (diff == 0).sum() / len(v_y)\n",
    "        print(f\"MODEL: lr ({nn.lr}), nh ({nn.nh})\")\n",
    "        print(\"valid fold:\", i, \"|\", \"valid acc:\", acc)\n",
    "    if(acc > best_pred[0]): best_pred = (acc, nn, res) \n",
    "    trends.append(best_pred[2])\n",
    "\n",
    "res = np.array(best_pred[2])\n",
    "epoch = np.arange(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
